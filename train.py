import gymnasium as gym
import torch
import numpy as np
import wandb
import os
import glob
import datetime
import module
import argparse
import yaml
from types import SimpleNamespace


parser = argparse.ArgumentParser()
parser.add_argument("--env", type=str, default="BipedalWalker-v3")
parser.add_argument("--resume_ckpt", type=str, default=None)
args = parser.parse_args()

# load configuration
cfg_path = f"config/{args.env}.yaml"
assert os.path.exists(cfg_path), f"config file not found: {cfg_path}"
with open(cfg_path) as f:
    cfg = yaml.safe_load(f)
cfg = SimpleNamespace(**cfg)

def compute_gae(rewards: list, values: list, gamma: float, lam: float):
    """
    Compute Generalized Advantage Estimation (GAE) for a single trajectory.

    Args:
        rewards (list of length T): Sequence of rewards for the trajectory.
        values (list of length T): Sequence of value estimates for each state.
        gamma (float): Discount factor.
        lam (float): GAE lambda parameter for bias-variance tradeoff.

    Returns:
        advantages (torch.Tensor with shape [T]): Estimated advantage at each timestep.
    """
    advantages = []
    gae = 0
    values = values.copy()
    values.append(0)
    for t in reversed(range(len(rewards))):
        delta = rewards[t] + gamma * values[t + 1] - values[t]
        gae = delta + gamma * lam * gae
        advantages.insert(0, gae)
    return torch.tensor(advantages, dtype=torch.float32)


# device setup.
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# parallel environments setup.
make_bipedal_env = lambda: gym.make(args.env)
envs = gym.vector.SyncVectorEnv([make_bipedal_env for _ in range(cfg.num_workers)])

obs_dim = envs.single_observation_space.shape[0]
act_dim = envs.single_action_space.shape[0]
max_episode_steps = envs.envs[0]._max_episode_steps

# initialize policy and optimizer.
network_class = getattr(module, cfg.network_class)
actor_critic = network_class(obs_dim, act_dim).to(device)
if args.resume_ckpt: # load checkpoint.
    # locate the best checkpoint
    extract_reward = lambda f: float(f.split("reward")[-1].replace(".pt", ""))
    ckpt_dir = os.path.join(cfg.ckpt_root_dir, args.env, args.resume_ckpt) 
    ckpts = sorted(glob.glob(os.path.join(ckpt_dir, "epoch*.pt")))
    assert ckpts, "No checkpoints found in directory."
    best_ckpt = max(ckpts, key=extract_reward)
    actor_critic.load_state_dict(torch.load(best_ckpt, map_location=device))
    actor_critic.eval()
    start_epoch = int(best_ckpt.split("epoch")[-1].split("_")[0]) + 1
    print(f"\nResuming from checkpoint {best_ckpt}. Starting epoch {start_epoch}.\n")
else: # train new policy.
    start_epoch = 0
optimizer = torch.optim.Adam(actor_critic.parameters(), lr=cfg.lr)

# initialize WandB logging and checkpoint folder.
run_time = datetime.datetime.now()
run_name = run_time.strftime("%m-%d-%y_%H:%M:%S")
wandb.init(project=f"PPO_{args.env}", name=run_name)

ckpt_run_dir = os.path.join(cfg.ckpt_root_dir, args.env, run_name)
os.makedirs(ckpt_run_dir, exist_ok=True)

for epoch in range(start_epoch, start_epoch + cfg.training_epochs):

    # initialize per-episode trajectory buffers.
    # each list stores a single trajectory and is cleared when the episode ends.
    obs_traj_buf = [[] for _ in range(cfg.num_workers)]    # observations
    act_traj_buf = [[] for _ in range(cfg.num_workers)]    # actions
    logp_traj_buf = [[] for _ in range(cfg.num_workers)]   # action log probabilities
    rew_traj_buf = [[] for _ in range(cfg.num_workers)]    # rewards
    val_traj_buf = [[] for _ in range(cfg.num_workers)]    # value estimates

    # initialize rollout data accumulators.
    obs_buf = []     # observations
    act_buf = []     # actions
    logp_buf = []    # action log probabilities
    adv_buf = []     # advantage estimates.
    ret_buf = []     # returns (real state values)

    # WandB outputs.
    episode_lengths = []
    episode_rewards = []

    # reset all environments at the start of the epoch.
    obs, info = envs.reset()

    # collect rollout data across parallel environments.
    for rollout_step in range(0, cfg.rollout_steps_per_epoch, cfg.num_workers):
        # sample actions and value estimates from current policy.
        with torch.no_grad():
            obs_cuda = torch.tensor(obs, dtype=torch.float32, device=device)
            action_dist, value = actor_critic(obs_cuda)
            action = action_dist.sample()
            action_logp = action_dist.log_prob(action).sum(dim=-1)
            action = action.cpu()

        # step environments with selected actions.
        next_obs, reward, terminated, truncated, info = envs.step(action.numpy())

        # save data by environments.
        for env in range(cfg.num_workers):
            obs_traj_buf[env].append(torch.from_numpy(obs[env]))
            act_traj_buf[env].append(action[env])
            logp_traj_buf[env].append(action_logp[env].item())
            rew_traj_buf[env].append(reward[env].item())
            val_traj_buf[env].append(value[env].item())

            # check if episode finished for this environment.
            done = terminated[env] or truncated[env]
            last_step = rollout_step + cfg.num_workers == cfg.rollout_steps_per_epoch

            if done or last_step:
                # episode finished or rollout ends â€” compute advantages and returns.
                rewards, values = rew_traj_buf[env], val_traj_buf[env]
                advantages = compute_gae(rewards, values, cfg.gamma, cfg.gae_lambda)
                returns = advantages + torch.tensor(values, dtype=torch.float32)

                # append collected trajectory to data accumulators.
                adv_buf.append(advantages)
                ret_buf.append(returns)
                act_buf.append(torch.stack(act_traj_buf[env]))
                obs_buf.append(torch.stack(obs_traj_buf[env]))
                logp_buf.append(torch.tensor(logp_traj_buf[env]))

                # record stats and clear trajectory buffers for this env.
                if done:
                    episode_lengths.append(len(rewards))
                    episode_rewards.append(sum(rewards))
                obs_traj_buf[env], act_traj_buf[env], logp_traj_buf[env], \
                    rew_traj_buf[env], val_traj_buf[env] = [], [], [], [], []

        # update current observations.
        obs = next_obs

        # reset terminated or truncated environments.
        done_mask = np.logical_or(terminated, truncated)
        if np.any(done_mask):
            obs, info = envs.reset(options={"reset_mask": done_mask})

    # consolidate data accumulators and move to GPU. N is rollout_step_per_epoch.
    # shapes: obs_buf [N, obs_dim]; act_buf [N, act_dim]; rest [N].
    obs_buf = torch.cat(obs_buf, dim=0).to(dtype=torch.float32, device=device) 
    act_buf = torch.cat(act_buf, dim=0).to(dtype=torch.float32, device=device)
    logp_buf = torch.cat(logp_buf, dim=0).to(dtype=torch.float32, device=device) 
    adv_buf = torch.cat(adv_buf, dim=0).to(dtype=torch.float32, device=device)
    ret_buf = torch.cat(ret_buf, dim=0).to(dtype=torch.float32, device=device)

    # update policy and value networks.
    for grad_step in range(cfg.grad_steps_per_update):
        # forward pass through policy network.
        action_dist, value = actor_critic(obs_buf)
        new_action_logp = action_dist.log_prob(act_buf).sum(dim=-1)

        # compute importance sampling ratio (new / old policy).
        action_prob_ratio = torch.exp(new_action_logp - logp_buf)

        # compute surrogate loss.
        adv_buf = (adv_buf - adv_buf.mean()) / (adv_buf.std() + 1e-8)
        surr1 = action_prob_ratio * adv_buf
        surr2 = torch.clamp(action_prob_ratio, 1 - cfg.clip_eps, 1 + cfg.clip_eps) * adv_buf
        policy_loss = -torch.min(surr1, surr2).mean()

        # value function loss (mean squared error).
        ret_buf = (ret_buf - ret_buf.mean()) / (ret_buf.std() + 1e-8)
        value = (value - value.mean()) / (value.std() + 1e-8)
        value_loss = ((ret_buf - value.squeeze()) ** 2).mean()

        # entropy bonus to encourage exploration.
        entropy_bonus = action_dist.entropy().sum(-1).mean()

        # combined loss.
        loss = policy_loss + cfg.value_loss_coef * value_loss - cfg.entropy_coef * entropy_bonus

        # backpropagation and optimizer step.
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(actor_critic.parameters(), cfg.max_grad_norm)
        optimizer.step()
    
    # log metrics to wandb
    reward_mean = sum(episode_rewards) / len(episode_rewards)
    episode_length_mean = sum(episode_lengths) / len(episode_lengths)

    wandb.log({
        "reward_mean": reward_mean,
        "episode_length_mean": episode_length_mean,
        "total_loss": loss.item(),
        "policy_loss": policy_loss.item(),
        "value_loss": value_loss.item(),
        "entropy_bonus": entropy_bonus.item(),
    }, step=epoch)

    print(f"epoch {epoch}/{start_epoch + cfg.training_epochs}:")
    print(f"reward_mean {reward_mean:.2f};\t total_loss {loss.item():.2f}.\n")

    # save model checkpoint.
    if epoch % cfg.ckpt_save_interval == 0:
        ckpts = sorted(glob.glob(os.path.join(ckpt_run_dir, "epoch*.pt")))
        ckpt_filename = f"epoch{epoch}_reward{reward_mean:.2f}.pt"
        ckpt_path = os.path.join(ckpt_run_dir, ckpt_filename)
        if len(ckpts) == 5: # only keep 5 checkpoints with highest rewards.
            extract_reward = lambda f: float(f.split("reward")[-1].replace(".pt", ""))
            worst_ckpt = min(ckpts, key=extract_reward)
            if reward_mean > extract_reward(worst_ckpt):
                os.remove(worst_ckpt)
                torch.save(actor_critic.state_dict(), ckpt_path)
        else:
              torch.save(actor_critic.state_dict(), ckpt_path)
        # save the current checkpoint as the most recent checkpoint.
        ckpt_path = os.path.join(ckpt_run_dir, "final_epoch.pt")
        torch.save(actor_critic.state_dict(), ckpt_path)